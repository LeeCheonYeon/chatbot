import re
import logging
import requests
from bs4 import BeautifulSoup
from qdrant_client import QdrantClient
from qdrant_client.http import models
from langchain_text_splitters  import RecursiveCharacterTextSplitter
from ollama import Client
from typing import List, Dict, Any
from datetime import datetime, timedelta

import hashlib
import pymysql
import time
import html

"""ì„¤ì • ì‹œì‘"""
EMBEDDING_URL = "http://localhost:7001/v1/embeddings"
EMBEDDING_MODEL = "BAAI/bge-m3"
QDRANT_URL = "http://localhost:7002"
RERANKER_URL = "http://localhost:8004"
COLLECTION_NAME = "test_cylee"
OLLAMA_MODEL = "llama3.1:8b"
OLLAMA_URL = "http://localhost:7003"

client = QdrantClient(url=QDRANT_URL)
ollama_client = Client(host=OLLAMA_URL)

conn = pymysql.connect(
    host="192.168.0.97",
	port=3336,
    user="csia",
    password="Csia##2024",
    database="csia",
    charset="utf8mb4",
    cursorclass=pymysql.cursors.DictCursor
)

# ë¡œê·¸ ì„¤ì •
logging.basicConfig(level=logging.INFO)

# í•œê¸€, ì˜ì–´, ìˆ«ì, ì¼ë³¸ì–´, í•œìë§Œ í—ˆìš©
strip_pattern = re.compile(
    r"[^ a-zA-Z0-9ê°€-í£ã„±-ã…ã…-ã…£ã-ã‚”ã‚¡-ãƒ´ãƒ¼ã€…ã€†ã€¤ä¸€-é¾¥]"
)
strip_pattern2 = re.compile(
    r"[^ a-zA-Z0-9ê°€-í£ã„±-ã…ã…-ã…£ã-ã‚”ã‚¡-ãƒ´ãƒ¼ã€…ã€†ã€¤ä¸€-é¾¥.:/()_-]]"
)
"""ì„¤ì • ë"""

# DBë·° ì¡°íšŒ
def load_from_view():
    with conn.cursor() as cur:
        cur.execute("""
            SELECT 
                PK_SEQ,
                TITLE,
                CONTENT,
                REG_DATE
            FROM vw_chatbot_contents
        """)
        return cur.fetchall()


#ì½œë ‰ì…˜ ìƒì„±(QDRANT)
def create_collection(collection_nm:str = COLLECTION_NAME) -> None:
    if client.collection_exists(collection_nm): 
        print(f"ğŸ—‘ï¸ ê¸°ì¡´ ì»¬ë ‰ì…˜ '{collection_nm}' ì‚­ì œ ì¤‘...")
        client.delete_collection(collection_name=COLLECTION_NAME)

    print(f"ğŸ“¦ ì»¬ë ‰ì…˜ '{collection_nm}' ìƒì„± ì¤‘...")
    vector_size = len(requests.post(
        EMBEDDING_URL,
        json={"input": "í…ŒìŠ¤íŠ¸", "model": EMBEDDING_MODEL},
        timeout=30
    ).json()["data"][0]["embedding"])
    
    if not client.collection_exists(collection_nm): 
        client.create_collection( collection_name=collection_nm, vectors_config=models.VectorParams(size=vector_size, distance=models.Distance.COSINE), )

#ì½œë ‰ì…˜ì— ì¸ë±ìŠ¤ ìƒì„±(QDRANT)
def create_index(collection_nm:str = COLLECTION_NAME, filed_nm:str = "") -> None:
    if not client.collection_exists(collection_nm): 
        print(f"ğŸ—‘ï¸ ì»¬ë ‰ì…˜ '{collection_nm}' ì¡´ì¬ í•˜ì§€ ì•ŠìŒ...")
    elif not filed_nm: 
        print("filed_nm ì¡´ì¬ í•˜ì§€ ì•ŠìŒ...")
    else:
        client.create_payload_index(
            collection_name=collection_nm,
            field_name=filed_nm,
            field_schema=models.TextIndexParams(
                type="text",
                tokenizer=models.TokenizerType.MULTILINGUAL, # í•œêµ­ì–´ í˜•íƒœì†Œ ëŒ€ì‘ì„ ìœ„í•´ í•„ìˆ˜
                min_token_len=2,
                max_token_len=20,
                lowercase=True,
            ) 
        )
        print(f"'{filed_nm}' í•„ë“œì— í…ìŠ¤íŠ¸ ì¸ë±ìŠ¤ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
             
#ì„ë² ë”© í…ìŠ¤íŠ¸ > ë²¡í„°(TEIì‚¬ìš©)
def get_embedding(text): 
    try:
        response = requests.post(EMBEDDING_URL, json={"model": EMBEDDING_MODEL, "input": text}, timeout=30 ) # ì„œë²„ ì‘ë‹µì—ì„œ ìˆ«ì ë¦¬ìŠ¤íŠ¸(ë²¡í„°)ë§Œ ë½‘ì•„ëƒ…ë‹ˆë‹¤.
        response.raise_for_status()
        return response.json()['data'][0]['embedding']
    except Exception as e:
        print(f"âŒ ì„ë² ë”© ì‹¤íŒ¨: {e}")
        return None

# ë¬¸ì„œ ì²­í¬ ì„ë² ë”© í›„ ì €ì¥
def make_chunk_id(post_id, chunk_index, text):
    base = f"{post_id}_{chunk_index}_{text[:50]}"
    return int(hashlib.md5(base.encode()).hexdigest()[:12], 16)

#ì½œë ‰ì…˜ì— ë°ì´í„° ì €ì¥(QDRANT)
def update_collection_data(collection_nm:str = COLLECTION_NAME, points:List[models.PointStruct] = []) ->None:
     if client.collection_exists(collection_nm) and points: 
        start_time = time.time()   # ì‹œì‘ ì‹œê°„ ê¸°ë¡
        BATCH_SIZE = 500  # í•œ ë²ˆì— ì—…ì„œíŠ¸í•  í¬ì¸íŠ¸ ê°œìˆ˜
        
        total_len = len(points)
        print(f"ğŸ“¦ ì´ {total_len}ê°œì˜ ë°ì´í„°ë¥¼ ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤. (ë°°ì¹˜ í¬ê¸°: {BATCH_SIZE})")
        for i in range(0, total_len, BATCH_SIZE):
            # 1. 500ê°œì”© ë°ì´í„° ìŠ¬ë¼ì´ì‹±
            batch_points = points[i : i + BATCH_SIZE]
            client.upsert(collection_name=COLLECTION_NAME, points=batch_points)
            print(f"ğŸ“¦ {i + len(batch_points)} / {total_len} ì™„ë£Œ...")
        print("âœ¨ ëª¨ë“  ë°°ì¹˜ê°€ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
     else:
        if not points:
            print("âš ï¸ ì—…ë¡œë“œí•  ë°ì´í„°(points)ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        else:
            print(f"âŒ '{collection_nm}' ì»¬ë ‰ì…˜ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë¨¼ì € ìƒì„±í•´ ì£¼ì„¸ìš”.")

#ì½œë ‰ì…˜ì— ë°ì´í„° ì €ì¥í• ë•Œ í•´ë‹¹ íƒ€ì…ì„ ë§ì¶°ì•¼ë˜ì„œ, ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
def trans_list_to_pointStructList(documents:List = [], type:str = 'A') -> List[models.PointStruct]:
    points = []
    if type == 'A':
        for i, doc in enumerate(documents): 
            vector = get_embedding(doc) 
            # ì‹¤ì œ í…ìŠ¤íŠ¸ ë¬¸ì¥ì„ í•¨ê»˜ ë³´ê´€ 
            points.append(models.PointStruct( id=i, vector=vector, payload={"text": doc} ))
    else:
        """í˜•ì‹ì— ë§ì¶°ì„œ êµ¬í˜„í•´ì•¼í•¨"""
        for i, doc in enumerate(documents):
            post_id = doc["PK_SEQ"]	
            title = doc["TITLE"] or ""
            content = doc["CONTENT"] or ""
            
            full_text = f"{title}\n{content}"
            full_text = html.unescape(full_text)
            bf_text = remove_tag_text(full_text)
            chunks = split_text(bf_text)
            for ii, chunk in enumerate(chunks):
                chunk_index = f"{i}_{ii}"
                id = make_chunk_id(post_id, chunk_index, chunk)
                vector = get_embedding(chunk)
                # ì‹¤ì œ í…ìŠ¤íŠ¸ ë¬¸ì¥ì„ í•¨ê»˜ ë³´ê´€ 
                points.append(models.PointStruct( id=id, vector=vector, payload={ "post_id": post_id,
                    "chunk_index": ii,
                    "text": chunk,
                    "full_contents":full_text,
                    "updated_at": str(doc["REG_DATE"])} ))
    return points

#ë²¡í„°ë¡œ ê²€ìƒ‰
def search_collection_data(collection_nm: str = COLLECTION_NAME, query_vector:list = None, count:int=5):
    if client.collection_exists(collection_nm) and query_vector: 
        search_result = client.query_points(collection_name=collection_nm, query=query_vector, limit=count )
        return search_result
    else:
        if not query_vector:
            print("âš ï¸ ë°ì´í„°(query_vector)ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        else:
            print(f"âŒ '{collection_nm}' ì»¬ë ‰ì…˜ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë¨¼ì € ìƒì„±í•´ ì£¼ì„¸ìš”.")
        return []

#ë²¡í„° ë° í‚¤ì›Œë“œë¡œ ê²€ìƒ‰
def search_collection_data_hybrid(collection_nm: str = COLLECTION_NAME, field_nm: str = "",vector_query_text:list = None, keyword_text:str = "", limit_count:int=5 ):
    if client.collection_exists(collection_nm) and vector_query_text and keyword_text: 
        # 2. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤í–‰
        search_result = client.query_points(
            collection_name=collection_nm,
            prefetch=[
                # (A) ë²¡í„° ê²€ìƒ‰: ì˜ë¯¸ì  ìœ ì‚¬ì„± ê¸°ë°˜ (2ë°°ìˆ˜ ì¶”ì¶œ)
                models.Prefetch(
                    query=vector_query_text,
                    limit=limit_count * 2,
                    score_threshold=0.5
                ),
                # (B) í‚¤ì›Œë“œ ê²€ìƒ‰: íŠ¹ì • ë‹¨ì–´ í¬í•¨ ì—¬ë¶€ ê¸°ë°˜ (2ë°°ìˆ˜ ì¶”ì¶œ)
                models.Prefetch(
                    # ì ìˆ˜ ê¸°ì¤€ì´ ì—†ì–´ë„ RRFê°€ ìˆœìœ„ë¥¼ ë§¤ê¸¸ ìˆ˜ ìˆë„ë¡ ë¹ˆ ë²¡í„°ë‚˜ ë”ë¯¸ë¥¼ ì£¼ì§€ ì•Šê³ ,
                    # í•„í„°ë§Œ ì ìš©ëœ Prefetch êµ¬ì¡°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
                    filter=models.Filter(
                        must=[
                            models.FieldCondition(
                                key=field_nm,
                                match=models.MatchText(text=keyword_text)
                            )
                        ]
                    ),
                    limit=limit_count * 2
                ),
            ],
            # (C) RRF ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ë‘ ê²°ê³¼ì˜ ìˆœìœ„ë¥¼ í•˜ë‚˜ë¡œ í†µí•©
            query=models.FusionQuery(fusion=models.Fusion.RRF),
            score_threshold=0.5,
            limit=limit_count  # ì „ë‹¬í•  ìµœì¢… í›„ë³´êµ° ê°œìˆ˜
        )
    return search_result

#ë¦¬ë­ì»¤ë¡œ ìˆœìœ„ ì •ë ¬
def get_rerank(query: str, documents: List[str]) -> List[Dict[str, Any]]:
    """
    ë¦¬ë­ì»¤ ì„œë²„ì— ì§ˆì˜í•˜ì—¬ ë¬¸ì„œë“¤ì˜ ì¬ìˆœìœ„í™”ëœ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Args:
        query: ì‚¬ìš©ì ì§ˆë¬¸ ë¬¸ìì—´
        documents: ê²€ìƒ‰ëœ ë¬¸ì„œ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        
    Returns:
        ì¬ìˆœìœ„ ì ìˆ˜ì™€ ì¸ë±ìŠ¤ ì •ë³´ê°€ ë‹´ê¸´ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: [{'index': 0, 'score': 0.9}, ...])
    """
    response = requests.post( f"{RERANKER_URL}/rerank", json={"query": query, "texts": documents} )
    return response.json()

#ë¦¬ë­ì»¤ë¡œ ìˆœìœ„ ì •ë ¬ í›„ ìƒìœ„nê°œë§Œ ë°˜í™˜
def get_refined_context(query: str, documents: List[str], top_n: int = 5,min_score: float = 0.5) -> Dict[str, Any]:
    """
    ë¦¬ë­ì»¤ ì„œë²„ì˜ ì‘ë‹µ í˜•ì‹([{"index": i, "score": s}, ...])ì„ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ë©°
    ìƒìœ„ Nê°œë§Œ ì˜ë¼ì„œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    if not documents:
        return []

    # 1. ë¦¬ë­ì»¤ ì„œë²„ì—ì„œ [{index: i, score: s}, ...] í˜•íƒœë¥¼ ê°€ì ¸ì˜´
    reranked_data = get_rerank(query, documents)
   # 2. í•„í„°ë§ ë° í…ìŠ¤íŠ¸ ë§¤ì¹­
    refined_results = []
    for item in reranked_data:
        if item['score'] >= min_score:
            idx = item['index']
            # ë¦¬ë­ì»¤ê°€ ì¤€ ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ì›ë³¸ documentsì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œ
            refined_results.append({
                "index": idx,
                "score": item['score'],
                "contexts": documents[idx]  # ğŸ‘ˆ ì—¬ê¸°ì— ì‹¤ì œ ë‚´ìš©ì„ ë„£ì–´ì¤ë‹ˆë‹¤!
            })
            
        # top_n ê°œìˆ˜ë§Œí¼ ì°¼ìœ¼ë©´ ì¤‘ë‹¨
        if len(refined_results) >= top_n:
            break
            
    return refined_results

def ask_ollama(context_text: str, user_query:str):
    # Ollamaì—ê²Œ ë³´ë‚¼ ìµœì¢… ë©”ì‹œì§€ êµ¬ì¡°
    final_prompt = f"""
    ì•„ë˜ [ì°¸ê³  ìë£Œ]ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•´ í•µì‹¬ê³¼ ìƒì„¸ ë‚´ìš©ì„ ì˜ ì¡°í•©í•˜ì—¬ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.

    [ì°¸ê³  ìë£Œ]
    {context_text}

    [ì‚¬ìš©ì ì§ˆë¬¸]
    {user_query}
    \n\ní•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•´ ì£¼ì„¸ìš”.
    ë‹µë³€:
    """

    response = ollama_client.chat(model=OLLAMA_MODEL, messages=[
        {
            "role": "system",
            "content": (
                "### ë‹µë³€ ê·œì¹™ ###\n"
                "0. **ì–¸ì–´**: í•œê¸€ë¡œ ë‹µë³€í•˜ì„¸ìš”. ì¶œë ¥ ì‹œ ì˜ì–´ ë‹¨ì–´ ì‚¬ìš©ì„ ìµœì†Œí™”í•˜ê³ , ëª¨ë“  ë¬¸ì¥ì„ ì™„ê²°ëœ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.\n"
                "1. **ë¬¸ì¥ í†µí•©**: í•µì‹¬ ê²°ë¡ ì„ ë¨¼ì € ì œì‹œí•˜ê³ , ë’¤ì´ì–´ êµ¬ì²´ì ì¸ ê·¼ê±°ë¥¼ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ ì´ì–´ ë¶™ì—¬ ë‹µë³€í•˜ë˜ ì¤‘ë³µëœ ë¬¸ì¥ì€ í”¼í•˜ì„¸ìš”.\n"
                "2. **ë…¼ë¦¬ì  ì—°ê²°**: 'ë”°ë¼ì„œ', 'êµ¬ì²´ì ìœ¼ë¡œëŠ”', 'ì´ì™€ ê´€ë ¨í•˜ì—¬', 'ê·¸ë¦¬ê³ ' ë“±ì˜ ì ‘ì†ì‚¬ë¥¼ ì‚¬ìš©í•˜ì—¬ í•µì‹¬ê³¼ ìƒì„¸ ì„¤ëª…ì´ í•˜ë‚˜ì˜ íë¦„ì´ ë˜ê²Œ í•˜ì„¸ìš”.\n"
                "3. **ì—„ê²©í•œ ê·¼ê±°**: ë°˜ë“œì‹œ ì œê³µëœ [ì°¸ê³  ìë£Œ]ì˜ ë‚´ìš©ë§Œ ì‚¬ìš©í•˜ê³ , ìë£Œì— ì—†ìœ¼ë©´ 'ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤'ë¼ê³  ë‹µí•˜ì„¸ìš”.\n"
                "4. **ë¬¸ì²´**: ì •ì¤‘í•œ ë¹„ì¦ˆë‹ˆìŠ¤ ê²½ì–´ì²´ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”\n"
            )
        },
        {
            'role': 'user',
            'content': final_prompt,
        },
       
    ],options={
            'temperature': 0,  # ì¼ê´€ëœ ë‹µë³€ì„ ìœ„í•´ ë‚®ê²Œ ì„¤ì •
            'num_ctx': 4096,
            'seed': 42,
        })
    return response['message']['content']

# ì§ˆë¬¸ì„ ê²€ìƒ‰ìš©ìœ¼ë¡œ ë³€ê²½
def rewrite_question(question):
   
    response = ollama_client.chat(model=OLLAMA_MODEL, messages=[
        {
            "role": "system",
            "content": (
                "### ì—­í•  ###\n"
                "ë„ˆëŠ” ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ 'ë²¡í„° ê²€ìƒ‰ìš© ë¬¸ì¥'ê³¼ 'í‚¤ì›Œë“œ ê²€ìƒ‰ìš© ë‹¨ì–´'ë¡œ ë³€í™˜í•˜ëŠ” ì „ë¬¸ ì¿¼ë¦¬ ìƒì„±ê¸°ì´ë‹¤.\n"

                "### ê·œì¹™ ###\n"
                "1. ì ˆëŒ€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µí•˜ì§€ ë§ˆë¼. (ì˜ˆ: '~ì…ë‹ˆë‹¤', '~í•˜ì„¸ìš”' ê¸ˆì§€)\n"
                "2. ì§ˆë¬¸ì— ì—†ëŠ” ì •ë³´ë¥¼ ìƒìƒí•´ì„œ ì¶”ê°€í•˜ì§€ ë§ˆë¼.\n"
                "3. ì¶œë ¥ì€ ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ì„ ì§€ì¼œë¼.\n"

                "### ì¶œë ¥ í˜•ì‹ ###\n"
                "ë¬¸ì¥: [ê²€ìƒ‰ì— ìµœì í™”ëœ ì™„ì„±í˜• ë¬¸ì¥] \n"
                "í‚¤ì›Œë“œ: [ê²€ìƒ‰ í•„í„°ë¡œ ì‚¬ìš©í•  í•µì‹¬ ëª…ì‚¬ë“¤, ì‰¼í‘œë¡œ êµ¬ë¶„]\n"

                "### ì˜ˆì‹œ ###\n"
                "ì…ë ¥: ê±°ê¸° ì–´ë–»ê²Œ ê°€ì•¼ í•˜ì§€?\n"
                "ë¬¸ì¥: í•´ë‹¹ ì¥ì†Œ ë°©ë¬¸ ë°©ë²• ë° ëŒ€ì¤‘êµí†µ ì˜¤ì‹œëŠ” ê¸¸ ì•ˆë‚´\n"
                "í‚¤ì›Œë“œ: ë°©ë¬¸ ë°©ë²•, ì˜¤ì‹œëŠ” ê¸¸, êµí†µí¸, ìœ„ì¹˜, ì§€ë„\n"
                "ì¶œë ¥ì€ ë°˜ë“œì‹œ ë¬¸ì¥:[ê²€ìƒ‰ì— ìµœì í™”ëœ ì™„ì„±í˜• ë¬¸ì¥]\ní‚¤ì›Œë“œ: [ê²€ìƒ‰ í•„í„°ë¡œ ì‚¬ìš©í•  í•µì‹¬ ëª…ì‚¬ë“¤, ì‰¼í‘œë¡œ êµ¬ë¶„] ë§Œ ë‚˜ì˜¤ê²Œ í•´ì¤˜"
            )
        },
        {
            'role': 'user',
            'content':  f"ì…ë ¥: {question}",
        },
       
    ],options={
           'temperature': 0,      # ëª¨ë¸ì˜ ëœë¤ì„±ì„ ì™„ì „íˆ ì œê±° (ê°€ì¥ ì¤‘ìš”)
            'num_ctx': 4096,       # ì»¨í…ìŠ¤íŠ¸ í¬ê¸° (í˜„ì¬ ì§ˆë¬¸ ì¬ì‘ì„±ì—ëŠ” ì¶©ë¶„í•¨)
            'seed': 42,            # ê²°ê³¼ ì¬í˜„ì„ ìœ„í•œ ì„¤ì •
            'num_predict': 50,     # ëª¨ë¸ì´ ë‚´ë±‰ëŠ” ê¸€ì ìˆ˜ë¥¼ ì œí•œ (ì‚¬ì¡± ë°©ì§€)
            'top_k': 1,  # ê°€ì¥ í™•ë¥ ì´ ë†’ì€ ë‹¨ì–´ 1ê°œë§Œ ê³ ë ¤
            'top_p': 1.0,
            'repeat_penalty': 1.0 # ë°˜ë³µ ë°©ì§€ ë¡œì§ì´ ê°œì…í•˜ì§€ ëª»í•˜ê²Œ í•¨
        })
    content = response['message']['content']
    # ë¬¸ì¥ê³¼ í‚¤ì›Œë“œ ë¶„ë¦¬
    vector_query = ""
    keyword_list = ""
    
    for line in content.split('\n'):
        if line.startswith("ë¬¸ì¥:"):
            vector_query = line.replace("ë¬¸ì¥:", "").strip()
        elif line.startswith("í‚¤ì›Œë“œ:"):
            keyword_list = line.replace("í‚¤ì›Œë“œ:", "").strip()
    return vector_query, keyword_list
        
               
#ë¬¸ì¥ ì „ì²˜ë¦¬(html,í•œê¸€íƒœê·¸ ë“± ì œê±°)
def remove_tag_text(raw_html: str) -> str:
    # HTML ë¶„ì„
    soup = BeautifulSoup(raw_html, "lxml")
    
    # ë³¸ë¬¸ê³¼ ìƒê´€ì—†ëŠ” íƒœê·¸(ìŠ¤í¬ë¦½íŠ¸, ìŠ¤íƒ€ì¼, ì£¼ì„ ë“±)ëŠ” ì•„ì˜ˆ ì‚­ì œ
    for extra in soup(["script", "style", "header", "footer", "nav"]):
        extra.decompose()
        
    # ëª¨ë“  ë§í¬(<a> íƒœê·¸)ë¥¼ ì°¾ì•„ "í…ìŠ¤íŠ¸(URL)" í˜•íƒœë¡œ ë³€í™˜
    for a in soup.find_all('a'):
        href = a.get('href', '').strip()
        link_text = a.get_text().strip()
        
        if href and not href.startswith('#'): # ë‚´ë¶€ ì´ë™ ì•µì»¤ ì œì™¸
            # í…ìŠ¤íŠ¸ì™€ ë§í¬ê°€ ë‹¤ë¥´ë©´ "í…ìŠ¤íŠ¸(ë§í¬)"ë¡œ, ê°™ìœ¼ë©´ í•˜ë‚˜ë§Œ í‘œì‹œ
            new_content = f" {link_text}({href}) " if link_text != href else f" {href} "
            a.replace_with(new_content)
        else:
            # ì£¼ì†Œê°€ ì—†ëŠ” ë§í¬ëŠ” í…ìŠ¤íŠ¸ë§Œ ë‚¨ê¹€
            a.replace_with(link_text)
        
    # í…ìŠ¤íŠ¸ ì¶”ì¶œ (íƒœê·¸ ê°„ ê°„ê²©ì„ ì£¼ì–´ ë‹¨ì–´ê°€ ë¶™ì§€ ì•Šê²Œ í•¨)
    text = soup.get_text(separator=" ")
    
    # HWP ì œì–´ ë¬¸ì ë“± ë¹„ì¸ì‡„ ë¬¸ì ì¶”ê°€ ì œê±°
    text = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f]', '', text)
    
    return text

def clean_for_reranker(text):
    # 1. ê´„í˜¸ ì•ˆì˜ URL ì œê±° (ë¦¬ë­ì»¤ëŠ” URLì„ ì½ì§€ ëª»í•¨)
    text = re.sub(r'\(http[s]?://\S+\)', '', text)
    # 2. ì¼ë°˜ URL ì œê±°
    text = re.sub(r'http[s]?://\S+', '', text)
    # 3. ì´ë©”ì¼/ì „í™”ë²ˆí˜¸ íƒœê·¸(tel:) ë“± íŠ¹ìˆ˜ íƒœê·¸ ì œê±°
    text = re.sub(r'\(tel:[^\)]+\)', '', text)
    # 4. ì—°ì†ëœ ê³µë°± í•˜ë‚˜ë¡œ í†µí•©
    text = " ".join(text.split())
    return text

# ë¬¸ì¥ ì „ì²˜ë¦¬(ì •ê·œì‹ìœ¼ë¡œ í•„ìš”í•œ ë¬¸ìë“¤ ë¹¼ê³  ì •ë¦¬)
def clean_text(text: str) -> str:
    #í—ˆìš©ëœ ë¬¸ìë§Œ ë‚¨ê¸°ê³  ê³µë°± ì •ë¦¬
    cleaned = strip_pattern2.sub("", text)
    return " ".join(cleaned.split())

# ë¬¸ì¥ ë¶„ë¦¬ , ë¬¸ì¥í´ë¦¬ë‹ ë° í•„í„°ë§, ë¬¸ì¥ë³‘í•©
def split_text(text: str) -> List[str]:
    #ë¬¸ì¥ ë¶„ë¦¬
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        separators=["\n\n", "\n", ". ", "? ", "! ", " ", ""]
    )

    chunks = []

    for chunk in text_splitter.split_text(text):
        #ë¬¸ì¥ í´ë¦¬ë‹ ë° í•„í„°ë§
        cleaned = clean_text(chunk)

        logging.info(f"cleaned: {cleaned}")

        if len(cleaned) >= 30:
            #ë¬¸ì¥ ë³‘í•©
            chunks.append(cleaned)

    return chunks

# 1. ì„œë²„ ë©”ëª¨ë¦¬ ì €ì¥ì†Œ (ìœ ì €ë³„ ëŒ€í™”ì™€ ë§ˆì§€ë§‰ í™œë™ ì‹œê°„ ì €ì¥)
# { "user_id": { "messages": [...], "last_activity": datetime } }
memory_store = {}

def get_refined_context(user_id):
    now = datetime.now()
    
    # í•´ë‹¹ ìœ ì €ì˜ ê¸°ë¡ì´ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    if user_id not in memory_store:
        return []
    
    user_data = memory_store[user_id]
    
    # 2. 30ë¶„ ì œí•œ ì²´í¬ (ë§ˆì§€ë§‰ í™œë™ìœ¼ë¡œë¶€í„° 30ë¶„ì´ ì§€ë‚¬ìœ¼ë©´ ë©”ëª¨ë¦¬ ì´ˆê¸°í™”)
    if now - user_data['last_activity'] > timedelta(minutes=30):
        memory_store[user_id] = {"messages": [], "last_activity": now}
        return []
    
    # 3. 3ê°œ ì œí•œ (ìµœì‹  ì§ˆë¬¸-ë‹µë³€ ì„¸íŠ¸ 3ê°œë§Œ ìœ ì§€)
    # ì§ˆë¬¸/ë‹µë³€ ìŒìœ¼ë¡œ ì €ì¥ë˜ë¯€ë¡œ ë¦¬ìŠ¤íŠ¸ ê¸¸ì´ëŠ” ìµœëŒ€ 6ê°œê°€ ë©ë‹ˆë‹¤.
    return user_data['messages']

def update_memory(user_id, user_query, assistant_answer):
    now = datetime.now()
    
    if user_id not in memory_store:
        memory_store[user_id] = {"messages": [], "last_activity": now}
    
    # ë©”ì‹œì§€ ì¶”ê°€
    memory_store[user_id]['messages'].append({"role": "user", "content": user_query})
    memory_store[user_id]['messages'].append({"role": "assistant", "content": assistant_answer})
    
    # 4. ë’¤ì—ì„œë¶€í„° 3ì„¸íŠ¸(6ê°œ ë©”ì‹œì§€)ë§Œ ë‚¨ê¸°ê³  ìë¥´ê¸°
    if len(memory_store[user_id]['messages']) > 6:
        memory_store[user_id]['messages'] = memory_store[user_id]['messages'][-6:]
    
    # ë§ˆì§€ë§‰ í™œë™ ì‹œê°„ ê°±ì‹ 
    memory_store[user_id]['last_activity'] = now